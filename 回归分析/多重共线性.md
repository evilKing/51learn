Tags: 回归分析篇

*多重共线性*
=========================

##什么是多重共线性

多元线性回归模型有一个基本假设，就是要求设计矩阵 $\boldsymbol{X}$ 的秩 $rank(\boldsymbol{X}) = p+1$ ，即要求 $\boldsymbol{X}$中的列向量之间线性无关。

如果存在不全为 $0$ 的 $p+1$ 个数 $c_0,c_1,c_2,\cdots,c_p$，使得 $$c_0 + c_1 x_{i1} + c_2 x_{i2} + \cdots + c_p x_{ip} = 0,\ \ \ \ i = 1,2,\cdots,n$$则自变量 $x_1,x_2,\cdots,x_p$ 之间存在着完全多重共线性。

______________________________________

在实际问题中完全的多重共线性并不多见，常见的是上式近似成立的情况，即存在不全为 $0$ 的 $p+1$个数 $c_0,c_1,c_2,\cdots,c_p$，使得$c_0,c_1,c_2,\cdots,c_p$，使得 $$c_0 + c_1 x_{i1} + c_2 x_{i2} + \cdots + c_p x_{ip} \approx 0,\ \ \ \ i = 1,2,\cdots,n$$

当自变量 $x_1,x_2,\cdots,x_p$ 存在上式的关系时，称自变量 $x_1,x_2,\cdots,x_p$ 之间存在着多重共线性(Multi-Collinearity)，也称为复共线性。

> 上述定义也可以作为是否存在多重共线性问题的检验方法

_______________________________________

##多重共线性产生的原因

解释变量之间完全不相关的情形是非常少见的，尤其是研究某个经济问题时，涉及的自变量较多，在建立回归模型时，往往由于研究者认识水平的局限性，我们很难找到一组自变量，它们之间互不相关，而且它们又都对因变量有显著影响.

客观地说，某一经济现象，涉及到多个影响因素时，这多个影响因素之间大都有一定的相关性。当它们之间的相关性较弱时，我们一般认为符合多元线性回归模型设计矩阵的要求；当这一组变量间有较弱的相关性时，我们就认为它违背了多元线性回归模型的基本假设.
_______________________________________

##多重共线性的检测

###方差扩大因子法
 
 对自变量作中心标准化，则 ${\boldsymbol{X}^*}'\boldsymbol{X}^* = (r_{ij})$ 为自变量的相关阵。记$$\boldsymbol{C} = (c_{ij}) = ({\boldsymbol{X}^*}'\boldsymbol{X}^*)^{-1}$$称其主对角线元素 $VIF_j = c_{jj}$ 为自变量 $x_j$ 的方差扩大因子(Variance Inflation Factor,简记为VIF)。
 
 因为$$var(\hat{\beta}_j) = L_{jj} c_{jj} \sigma^2,\ \ j = 1,2,\cdots,p$$其中，$L_{jj}$为$x_j$的离差平方和，用 $c_{jj}$ 作为衡量自变量 $x_j$的方差扩大程度的因子是恰如其分的。
 
 记 $\boldsymbol{R}_j^2$ 为自变量 $x_j$ 对其余 $p-1$个自变量的复决定系数，可以证明 $$c_{jj} = \frac{1}{1-\boldsymbol{R}_j^2}$$同样也可以作为方差扩大因子 $VIF_j$ 的定义，由此式可知 $VIF_j \geq 1$。
 
 ________________________________________
 
 由于 $\boldsymbol{R}_j^2$度量了自变量 $x_j$与其余 $p-1$个自变量的线性相关程度，这种相关程度越强，说明自变量之间的多重共线性越严重，$\boldsymbol{R}_j^2$也就越接近于1，$VIF_j$也就越大；反之，$x_j$与其余 $p-1$个自变量线性相关程度越弱，自变量间的多重共线性也就越弱，$\boldsymbol{R}_j^2$就越接近于零，$VIF_j$ 也就越接近于 1.
 
 由此可见 $VIF_j$ 的大小反映了自变量之间是否存在多重共线性，因此可由它来度量多重共线性的严重程度。
 
经验表明，当 $VIF_j \geq 10$ 时，就说明自变量 $x_j$ 与其余自变量之间存在严重的多重共线性，且这种多重共线性可能会过度地影响最小二乘估计值.

> 有说也可以用$$\overline{VIF} = \frac{1}{p} \sum_{j=1}^p{VIF_j}$$来度量多重共线性，当该值远远大于 $1$ 时就表示存在严重的多重共线性问题.

___________________________________________

##直观判定法

1. 当增加或剔除一个自变量，或者改变一个观测值时，回归系数的估计值发生较大变化，我们就认为回归方程存在严重的多重共线性.

2. 从定性分析认为，一些重要的自变量在回归方程中没有通过显著性检验时，可初步判断存在严重的多重共线性.

3. 有些自变量的回归系数所带正负号与定性分析结果违背时，我们认为存在多重共线性问题.

4. 自变量的相关矩阵中，自变量间的相关系数较大时，我们认为可能会出现多重共线性的问题.

5. 一些重要的自变量的回归系数的标准误差较大时，我们认为可能存在多重共线性.

________________________________________

##消除多重共线性

###剔除一些不重要的解释变量

通常在经济问题的建模中，由于我们认识水平的局限，容易考虑过多的自变量。当涉及自变量较多时，大多数回归方程都受到多重共线性的影响。

这时，最常用的方法是先用上篇[自变量的选取](https://www.zybuluo.com/evilking/note/761077)所述的方法，舍去一些自变量。当回归方程中的全部自变量都通过显著性检验后，回归方程中任然存在严重的多重共线性，有几个变量的方差扩大因子大于$10$，我们可以把方程扩大因子最大者所对应的自变量首先剔除，再重新建立回归方程，如果任然存在严重的多重共线性，则再继续剔除方差扩大因子最大者所对应的自变量，直到回归方程中不再存在严重的多重共线性为止。

在选择回归模型时，可以将回归系数的显著性检验、方差扩大因子$VIF$的多重共线性检验与自变量的经济含义结合起来考虑，以改进或剔除变量.

_________________________________________

###增大样本容量

建立一个实际问题的回归模型，如果所收集的样本数据太少，也容易产生多重共线性。

譬如，我们的问题涉及到两个自变量$x_1,x_2$，假设 $x_1,x_2$都已经中心化了，因为$$var(\hat{\beta}_1) = \frac{\sigma^2}{(1-r_{12}^2) L_{11}} \\ var(\hat{\beta}_2) = \frac{\sigma^2}{(1-r_{12}^2) L_{22}} $$其中，$r_{12}$为$x_1,x_2$的相关系数，$L_{11} = \sum_{i=1}^n{x_{i1}^2}, L_{22} = \sum_{i=1}^n{x_{i2}^2}$

可以看到，在 $r_{12}$固定不变时，当样本容量 $n$ 增大时，$L_{11}$和 $L_{22}$都会增大，两个方差均可减小，从而减弱了多重共线性对回归方程的影响.

因此，增大样本容量也是消除多重共线性的一个途径。当我们所选的变量个数接近样本容量 $n$时，自变量间就容易产生共线性，所以在实际的问题研究中，要尽可能使样本容量 $n$远大于自变量个数 $p$.

> 在实际的问题中，由于有些时候获取样本的代价非常昂贵，增加样本数量变得很困难，所以这种方法并不总是有效；同时在有些情况下，增加了样本数据，还可能引起其他一些新的问题，使模型拟合变差

____________________________________________

###回归系数的有偏估计

消除多重共线性对回归模型的影响，统计学家们研究已久，除上述常用方法外，统计学家们还致力于改进古典的最小二乘法，提出以采用有偏估计为代价来提高估计量稳定性的方法，如岭回归法、主成分法、偏最小二乘法等，现在已有很多应用效果不错的例子

这里我们只介绍上述一些简单常用的方法，像岭回归法等复杂的方法读者可以网上自行查询学习.

________________________________________

##R程序演示

